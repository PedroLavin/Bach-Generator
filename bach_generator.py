# -*- coding: utf-8 -*-
"""Bach_Generator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i-1IRMYa20tT1AOBTY1RRe2UxE5nZb6n
"""

!pip install pretty_midi music21 torch numpy

# Core ML
import tensorflow as tf  # or PyTorch
import numpy as np

# Music processing
from music21 import stream, note, pitch, duration, midi
import pretty_midi

# Data handling
import pandas as pd

import os
from music21 import converter, stream, note, chord, duration, key, meter
import glob

# Set path to your MIDI files
midi_folder = "bach_completion_project/midi_files"  # adjust path if needed

def analyze_midi_collection(folder_path):
    """Analyze the MIDI files to understand our dataset"""
    midi_files = glob.glob(os.path.join(folder_path, "*.mid")) + \
                 glob.glob(os.path.join(folder_path, "*.midi"))

    print(f"Found {len(midi_files)} MIDI files")
    print("\nAnalyzing files...")

    for i, file_path in enumerate(midi_files[:5]):  # Analyze first 5 files
        try:
            # Load the MIDI file
            score = converter.parse(file_path)
            filename = os.path.basename(file_path)

            print(f"\n--- File {i+1}: {filename} ---")
            print(f"Duration: {score.duration.quarterLength} quarter notes")
            print(f"Number of parts: {len(score.parts)}")

            # Get key signature if available
            key_sig = score.analyze('key')
            print(f"Key: {key_sig}")

            # Count notes/chords
            notes = score.flat.notes
            print(f"Total notes/chords: {len(notes)}")

            # Show first few notes
            print("First few elements:")
            for j, element in enumerate(notes[:10]):
                if isinstance(element, note.Note):
                    print(f"  Note: {element.pitch} (duration: {element.duration.quarterLength})")
                elif isinstance(element, chord.Chord):
                    print(f"  Chord: {[str(p) for p in element.pitches]} (duration: {element.duration.quarterLength})")

        except Exception as e:
            print(f"Error processing {filename}: {e}")

    return midi_files

# Run the analysis
midi_files = analyze_midi_collection(midi_folder)

"""# **Preprocessing**"""

import os
import glob
import pickle
from music21 import converter, note, chord, stream
import numpy as np

def extract_notes_from_midi(file_path):
    """Extract notes and chords from a MIDI file"""
    try:
        midi = converter.parse(file_path)
        notes_to_parse = []

        # Handle multi-part pieces by flattening
        if len(midi.parts) > 1:
            # For polyphonic pieces, we'll focus on the melody line (usually the highest part)
            # or flatten all parts together
            notes_to_parse = midi.flat.notes
        else:
            notes_to_parse = midi.flat.notes

        notes = []
        for element in notes_to_parse:
            if isinstance(element, note.Note):
                # Single note: store pitch
                notes.append(str(element.pitch))
            elif isinstance(element, chord.Chord):
                # Chord: store as joined pitches
                notes.append('.'.join(str(n) for n in element.normalOrder))

        return notes
    except Exception as e:
        print(f"Error processing {file_path}: {e}")
        return []

def create_sequences(notes, sequence_length=100):
    """Create input-output pairs for training"""
    # Get unique notes for vocabulary
    vocabulary = sorted(set(notes))
    print(f"Vocabulary size: {len(vocabulary)} unique notes/chords")

    # Create note-to-integer mapping
    note_to_int = dict((note, number) for number, note in enumerate(vocabulary))
    int_to_note = dict((number, note) for number, note in enumerate(vocabulary))

    # Create sequences
    network_input = []
    network_output = []

    # Create sequences of specified length
    for i in range(0, len(notes) - sequence_length, 1):
        sequence_in = notes[i:i + sequence_length]
        sequence_out = notes[i + sequence_length]

        network_input.append([note_to_int[char] for char in sequence_in])
        network_output.append(note_to_int[sequence_out])

    n_patterns = len(network_input)
    print(f"Created {n_patterns} training sequences")

    return network_input, network_output, vocabulary, note_to_int, int_to_note

# Process all MIDI files
def process_all_files(midi_folder):
    """Process all MIDI files and extract training data"""
    midi_files = glob.glob(os.path.join(midi_folder, "*.mid")) + \
                 glob.glob(os.path.join(midi_folder, "*.midi"))

    print(f"Processing {len(midi_files)} MIDI files...")

    all_notes = []

    for file_path in midi_files:
        print(f"Processing: {os.path.basename(file_path)}")
        notes = extract_notes_from_midi(file_path)
        if notes:  # Only add if we successfully extracted notes
            all_notes.extend(notes)
            print(f"  Extracted {len(notes)} notes")

    print(f"\nTotal notes collected: {len(all_notes)}")

    if len(all_notes) < 1000:
        print("Warning: Very few notes extracted. You might need more MIDI files.")

    return all_notes

# Run the preprocessing
midi_folder = "bach_completion_project/midi_files"
all_notes = process_all_files(midi_folder)

# Create training sequences
if len(all_notes) > 0:
    network_input, network_output, vocabulary, note_to_int, int_to_note = create_sequences(all_notes)

    # Save the processed data
    with open('training_data.pkl', 'wb') as f:
        pickle.dump({
            'network_input': network_input,
            'network_output': network_output,
            'vocabulary': vocabulary,
            'note_to_int': note_to_int,
            'int_to_note': int_to_note,
            'all_notes': all_notes
        }, f)

    print(f"\nPreprocessing complete!")
    print(f"Vocabulary size: {len(vocabulary)}")
    print(f"Training sequences: {len(network_input)}")
    print(f"Sequence length: 100 notes each")
    print("Data saved to 'training_data.pkl'")
else:
    print("No notes were extracted. Check your MIDI files.")

"""# **Network Definition**"""

import pickle
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, LSTM, Activation
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import ModelCheckpoint
import tensorflow as tf

# Load the preprocessed data
print("Loading training data...")
with open('training_data.pkl', 'rb') as f:
    data = pickle.load(f)

network_input = data['network_input']
network_output = data['network_output']
vocabulary = data['vocabulary']
note_to_int = data['note_to_int']
int_to_note = data['int_to_note']

print(f"Loaded {len(network_input)} training sequences")
print(f"Vocabulary size: {len(vocabulary)}")

# Prepare the data for the neural network
def prepare_sequences(network_input, network_output, n_vocab):
    """Prepare sequences for training"""
    # Reshape input for LSTM [samples, time steps, features]
    sequence_length = len(network_input[0])
    network_input = np.reshape(network_input, (len(network_input), sequence_length, 1))

    # Normalize input (divide by vocabulary size)
    network_input = network_input / float(n_vocab)

    # Convert output to categorical (one-hot encoded)
    network_output = to_categorical(network_output)

    return network_input, network_output

# Prepare training data
n_vocab = len(vocabulary)
network_input, network_output = prepare_sequences(network_input, network_output, n_vocab)

print(f"Input shape: {network_input.shape}")
print(f"Output shape: {network_output.shape}")

# Build the neural network model
def create_network(network_input, n_vocab):
    """Create the neural network model"""
    model = Sequential()

    # First LSTM layer
    model.add(LSTM(
        512,  # Number of LSTM units
        input_shape=(network_input.shape[1], network_input.shape[2]),
        return_sequences=True,
        recurrent_dropout=0.3
    ))

    # Second LSTM layer
    model.add(LSTM(512, return_sequences=True, recurrent_dropout=0.3))

    # Third LSTM layer
    model.add(LSTM(512, recurrent_dropout=0.3))

    # Dropout for regularization
    model.add(Dropout(0.3))

    # Output layer
    model.add(Dense(n_vocab))
    model.add(Activation('softmax'))

    # Compile the model
    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')

    return model

# Create the model
print("Creating neural network model...")
model = create_network(network_input, n_vocab)

# Print model summary
print("\nModel Architecture:")
model.summary()

# Set up model checkpoint to save best weights
checkpoint = ModelCheckpoint(
    'weights-improvement-{epoch:02d}-{loss:.4f}-bigger.h5',
    monitor='loss',
    verbose=0,
    save_best_only=True,
    mode='min'
)

callbacks_list = [checkpoint]

print(f"\nModel ready for training!")
print(f"Training data shape: {network_input.shape}")
print(f"Output shape: {network_output.shape}")
print(f"The model will learn to predict the next note from sequences of 100 previous notes")

# Save model architecture info
model_info = {
    'n_vocab': n_vocab,
    'vocabulary': vocabulary,
    'note_to_int': note_to_int,
    'int_to_note': int_to_note
}

with open('model_info.pkl', 'wb') as f:
    pickle.dump(model_info, f)

print("Model information saved to 'model_info.pkl'")
print("\nReady to start training! The model architecture is complete.")

# Continue from previous script or load everything again
import pickle
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, LSTM, Activation
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import ModelCheckpoint

# If starting fresh, uncomment and run the data loading:
"""
with open('training_data.pkl', 'rb') as f:
    data = pickle.load(f)

network_input = data['network_input']
network_output = data['network_output']
vocabulary = data['vocabulary']
note_to_int = data['note_to_int']
int_to_note = data['int_to_note']

# Prepare sequences (same as before)
sequence_length = len(network_input[0])
network_input = np.reshape(network_input, (len(network_input), sequence_length, 1))
network_input = network_input / float(len(vocabulary))
network_output = to_categorical(network_output)

# Recreate model (same as before)
model = Sequential()
model.add(LSTM(512, input_shape=(network_input.shape[1], network_input.shape[2]), return_sequences=True, recurrent_dropout=0.3))
model.add(LSTM(512, return_sequences=True, recurrent_dropout=0.3))
model.add(LSTM(512, recurrent_dropout=0.3))
model.add(Dropout(0.3))
model.add(Dense(len(vocabulary)))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop')
"""

# Training configuration
EPOCHS = 5  # Start with 50 epochs (you can increase later)
BATCH_SIZE = 64

print("Starting training...")
print(f"Training for {EPOCHS} epochs with batch size {BATCH_SIZE}")
print("This will take some time (potentially 1-3 hours depending on your hardware)")
print("You'll see the loss decrease as the model learns Bach's patterns")

# Set up checkpoint to save the best model
checkpoint = ModelCheckpoint(
    'best_weights.h5',
    monitor='loss',
    verbose=1,
    save_best_only=True,
    mode='min'
)

# Train the model
history = model.fit(
    network_input,
    network_output,
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    callbacks=[checkpoint],
    verbose=1  # Shows progress bar and loss for each epoch
)

print("\nTraining completed!")
print("Best model weights saved to 'best_weights.hdf5'")

# Save the final model as well
model.save('final_model.h5')
print("Final model saved to 'final_model.h5'")

# Show training progress
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'])
plt.title('Model Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.show()

print(f"Final training loss: {history.history['loss'][-1]:.4f}")
print("Lower loss = better learning of Bach's patterns")

